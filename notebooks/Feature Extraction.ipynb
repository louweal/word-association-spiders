{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/wordgame_20170807.csv')\n",
    "df['word'] = df['word'].astype(str)\n",
    "df['association'] = df['association'].astype(str)\n",
    "\n",
    "# later\n",
    "with open('../data/processed/sources.csv') as f:\n",
    "    sources_list = f.read().splitlines()\n",
    "print(sources_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape: \" + str(df.shape))\n",
    "print(\"Number of sources: \" + str(len(df['forumID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic features\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tf'] = (100*df.groupby(['word'])['word'].transform('count'))/len(df) #percentage\n",
    "print(df.word.value_counts().head(7).index.tolist())\n",
    "print(\"Mean: \"+str(df.tf.mean())+\"\\tMedian: \"+str(df.tf.median()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pair'] = df.apply(lambda r: str(r.word) + \":\" + str(r.association), axis=1)\n",
    "df['pf'] = (100*df.groupby(['pair'])['pair'].transform('count'))/len(df)\n",
    "df.pair.value_counts().head(7).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len1'] = df['word'].apply(lambda x:len(x))\n",
    "df['len2'] = df['association'].apply(lambda x:len(x))\n",
    "df['ldiff'] = df['len1'] - df['len2'] # length difference\n",
    "print(\"Mean: \"+str(df.len1.mean())+\"\\tMedian: \"+str(df.len1.median()))  \n",
    "print(\"Mean: \"+str(df.ldiff.mean())+\"\\tMedian: \"+str(df.ldiff.median()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "df['levenshtein'] = df.apply(lambda r:Levenshtein.distance(r.word, r.association), axis=1)\n",
    "print(\"Mean: \"+str(df.levenshtein.mean())+\"\\tMedian: \"+str(df.levenshtein.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "df['prefix'] = df.apply(lambda r: os.path.commonprefix([r.word, r.association]), axis=1)\n",
    "df['pl']= (100*df['prefix'].apply(lambda x: len(x)))/(0.5*(df['len1']+df['len2']))\n",
    "df['suffix'] = df.apply(lambda r: os.path.commonprefix([r.word[::-1], r.association[::-1]]), axis=1)\n",
    "df['suffix'] = df['suffix'].apply(lambda x:x[::-1]) #re-reverse suffix\n",
    "df['sl']= (100*df['suffix'].apply(lambda x: len(x)))/(0.5*(df.len1+df.len2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.prefix.value_counts().head(20).index.tolist())\n",
    "print(\"Mean: \"+str(df.pl.mean())+\"\\tMedian: \"+str(df.pl.median()))\n",
    "print(df.suffix.value_counts().head(20).index.tolist())\n",
    "print(\"Mean: \"+str(df.sl.mean())+\"\\tMedian: \"+str(df.sl.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2v_model = KeyedVectors.load_word2vec_format('../data/external/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print('Loaded word embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inw2v'] = df.apply(lambda r:((r.word in w2v_model.vocab) & (r.association in w2v_model.vocab)), axis=1)\n",
    "print(\"Mean: \"+str(df.inw2v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sim'] = 0\n",
    "df.ix[df.inw2v, 'sim'] = df.ix[df.inw2v].apply(lambda r:w2v_model.similarity(r.word, r.association), axis=1)\n",
    "print(\"Mean: \"+str(df.sim.mean())+\"\\tMedian: \"+str(df.sim.median()))\n",
    "print(df[(df.sim<0.18)&(df.sim>0.17)].pair.head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wv1'] = df['word'].apply(lambda x: np.zeros(300)) \n",
    "df['wv2'] = df['word'].apply(lambda x: np.zeros(300)) \n",
    "\n",
    "df.ix[df.inw2v, 'wv1'] = df.ix[df.inw2v, 'word'].apply(lambda x: w2v_model.word_vec(x)) \n",
    "df.ix[df.inw2v, 'wv2'] = df.ix[df.inw2v, 'association'].apply(lambda x: w2v_model.word_vec(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Synset.lemma_names of Synset('canine.n.02')>\n",
      "<bound method Synset.lemma_names of Synset('domestic_animal.n.01')>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "dog = wn.synset(\"dog.n.01\")\n",
    "for h in dog.hypernyms():\n",
    "    print(h.lemma_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
