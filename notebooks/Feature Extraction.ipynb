{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AC', 'BC', 'CC', 'ECF', 'GOG', 'LEF', 'SAS', 'TF', 'U2', 'WP']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>forum</th>\n",
       "      <th>word</th>\n",
       "      <th>association</th>\n",
       "      <th>forumID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My smile is relief</td>\n",
       "      <td>U2</td>\n",
       "      <td>children</td>\n",
       "      <td>cute</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Habanerose</td>\n",
       "      <td>GOG</td>\n",
       "      <td>journey</td>\n",
       "      <td>adventure</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Judge</td>\n",
       "      <td>AC</td>\n",
       "      <td>listerine</td>\n",
       "      <td>antiseptic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kleetus</td>\n",
       "      <td>GOG</td>\n",
       "      <td>whale</td>\n",
       "      <td>hello</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>le_chevalier</td>\n",
       "      <td>GOG</td>\n",
       "      <td>no</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user forum       word association  forumID\n",
       "0  My smile is relief    U2   children        cute        8\n",
       "1          Habanerose   GOG    journey   adventure        4\n",
       "2               Judge    AC  listerine  antiseptic        0\n",
       "3             Kleetus   GOG      whale       hello        4\n",
       "4        le_chevalier   GOG         no  conspiracy        4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/wordgame_20170807.csv')\n",
    "df['word'] = df['word'].astype(str)\n",
    "df['association'] = df['association'].astype(str)\n",
    "\n",
    "# later\n",
    "with open('../data/processed/sources.csv') as f:\n",
    "    sources_list = f.read().splitlines()\n",
    "print(sources_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (330395, 5)\n",
      "Number of sources: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape: \" + str(df.shape))\n",
    "print(\"Number of sources: \" + str(len(df['forumID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic features\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water', 'music', 'time', 'money', 'love', 'fire', 'food']\n",
      "Mean: 0.0188537006299\tMedian: 0.007869368483179226\n"
     ]
    }
   ],
   "source": [
    "df['tf'] = (100*df.groupby(['word'])['word'].transform('count'))/len(df) #percentage\n",
    "print(df.word.value_counts().head(7).index.tolist())\n",
    "print(\"Mean: \"+str(df.tf.mean())+\"\\tMedian: \"+str(df.tf.median()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me:you',\n",
       " 'man:woman',\n",
       " 'up:down',\n",
       " 'time:clock',\n",
       " 'meow:meow',\n",
       " 'green:grass',\n",
       " 'dog:cat']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pair'] = df.apply(lambda r: str(r.word) + \":\" + str(r.association), axis=1)\n",
    "df['pf'] = (100*df.groupby(['pair'])['pair'].transform('count'))/len(df)\n",
    "df.pair.value_counts().head(7).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 6.77352865509\tMedian: 6.0\n",
      "Mean: -0.000853523812406\tMedian: 0.0\n"
     ]
    }
   ],
   "source": [
    "df['len1'] = df['word'].apply(lambda x:len(x))\n",
    "df['len2'] = df['association'].apply(lambda x:len(x))\n",
    "df['ldiff'] = df['len1'] - df['len2'] # length difference\n",
    "print(\"Mean: \"+str(df.len1.mean())+\"\\tMedian: \"+str(df.len1.median()))  \n",
    "print(\"Mean: \"+str(df.ldiff.mean())+\"\\tMedian: \"+str(df.ldiff.median()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 6.80292982642\tMedian: 6.0\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "df['levenshtein'] = df.apply(lambda r:Levenshtein.distance(r.word, r.association), axis=1)\n",
    "print(\"Mean: \"+str(df.levenshtein.mean())+\"\\tMedian: \"+str(df.levenshtein.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "df['prefix'] = df.apply(lambda r: os.path.commonprefix([r.word, r.association]), axis=1)\n",
    "df['pl']= (100*df['prefix'].apply(lambda x: len(x)))/(0.5*(df['len1']+df['len2']))\n",
    "df['suffix'] = df.apply(lambda r: os.path.commonprefix([r.word[::-1], r.association[::-1]]), axis=1)\n",
    "df['suffix'] = df['suffix'].apply(lambda x:x[::-1]) #re-reverse suffix\n",
    "df['sl']= (100*df['suffix'].apply(lambda x: len(x)))/(0.5*(df.len1+df.len2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 's', 'c', 'b', 'p', 'm', 'f', 't', 'd', 'a', 'w', 'h', 'r', 'g', 'l', 'e', 're', 'co', 'n', 'i']\n",
      "Mean: 3.40372050678\tMedian: 0.0\n",
      "['', 'e', 's', 't', 'y', 'n', 'ed', 'er', 'ing', 'r', 'd', 'es', 'l', 'a', 'tion', 'k', 'ion', 'on', 'o', 'ation']\n",
      "Mean: 4.8668958518\tMedian: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(df.prefix.value_counts().head(20).index.tolist())\n",
    "print(\"Mean: \"+str(df.pl.mean())+\"\\tMedian: \"+str(df.pl.median()))\n",
    "print(df.suffix.value_counts().head(20).index.tolist())\n",
    "print(\"Mean: \"+str(df.sl.mean())+\"\\tMedian: \"+str(df.sl.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word embeddings\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2v_model = KeyedVectors.load_word2vec_format('../data/external/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print('Loaded word embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.819273899423\n"
     ]
    }
   ],
   "source": [
    "df['inw2v'] = df.apply(lambda r:((r.word in w2v_model.vocab) & (r.association in w2v_model.vocab)), axis=1)\n",
    "print(\"Mean: \"+str(df.inw2v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['sim'] = 0\n",
    "df.ix[df.inw2v, 'sim'] = df.ix[df.inw2v].apply(lambda r:w2v_model.similarity(r.word, r.association), axis=1)\n",
    "print(\"Mean: \"+str(df.sim.mean())+\"\\tMedian: \"+str(df.sim.median()))\n",
    "print(df[(df.sim<0.18)&(df.sim>0.17)].pair.head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['wv1'] = df['word'].apply(lambda x: np.zeros(300)) \n",
    "df['wv2'] = df['word'].apply(lambda x: np.zeros(300)) \n",
    "\n",
    "df.ix[df.inw2v, 'wv1'] = df.ix[df.inw2v, 'word'].apply(lambda x: w2v_model.word_vec(x)) \n",
    "df.ix[df.inw2v, 'wv2'] = df.ix[df.inw2v, 'association'].apply(lambda x: w2v_model.word_vec(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'luminosity', 'light_up', 'tripping', 'spark', 'Light', 'faint', 'visible_radiation', 'luminance', 'illumine', 'visible_light', 'illuminate', 'brightness', 'lightsome', 'clear', 'sparkle', 'weak', 'get_down', 'promiscuous', 'fire_up', 'twinkle', 'dismount', 'abstemious', 'short', 'fall', 'unaccented', 'ignitor', 'perch', 'easy', 'lightly', 'light', 'wanton', 'idle', 'light-colored', 'Christ_Within', 'lighter', 'lightness', 'Inner_Light', 'clean', 'lighting', 'igniter', 'ignite', 'calorie-free', 'light-headed', 'light_source', 'brightness_level', 'scant', 'swooning', 'sluttish', 'luminousness', 'lite', 'illume', 'Light_Within', 'alight', 'unhorse', 'illumination', 'wakeful', 'unclouded', 'lightheaded', 'low-cal', 'get_off', 'loose'}\n",
      "{'heavy', 'extinguish', 'dark'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "#word = wn.synset(\"water.n.01\")\n",
    "\n",
    "#for i in word.hypernyms():\n",
    "#    print(i.name())\n",
    "#for i in word.hyponyms():\n",
    "#    print(i.name())    \n",
    "    \n",
    "for syn in wn.synsets(\"light\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name()) #not it!\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "            \n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop('wv1', 1)\n",
    "#df = df.drop('wv2', 1)\n",
    "df.to_csv(\"../data/processed/wordgame_20170807_ft.csv\", sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
